{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB = os.environ.get('JOB')\n",
    "TASK = int(os.environ.get('TASK'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ps': ['tf-ps-0:2222', 'tf-ps-1:2222'],\n",
       " 'worker': ['tf-worker-0:2222',\n",
       "  'tf-worker-1:2222',\n",
       "  'tf-worker-2:2222',\n",
       "  'tf-worker-3:2222'],\n",
       " 'master': ['tf-master-0:2222']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/eslap/legacy/cluster.json','r') as f:\n",
    "        cluster=json.load(f)\n",
    "        cluster_spec=tf.train.ClusterSpec(cluster)\n",
    "        \n",
    "workers = ['/job:worker/task:'+str(i) for i in range(len(cluster['worker']))]\n",
    "param_servers = ['/job:ps/task:'+str(i) for i in range(len(cluster['ps']))]\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = tf.train.Server(cluster_spec, job_name=JOB, task_index=TASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=pd.read_csv('/eslap/legacy/mnist.csv')\n",
    "train,test=train_test_split(all_data,test_size=0.25)\n",
    "del all_data \n",
    "batch_size=128\n",
    "num_batches=int(train.shape[0]/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(targets,num_classes):\n",
    "    aux = np.zeros((targets.shape[0], num_classes))\n",
    "    aux[np.arange(targets.shape[0]), targets] = 1\n",
    "    return aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31500, 784), (31500, 10), (10500, 784), (10500, 10))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = to_one_hot(train['label'].values, num_classes = 10)\n",
    "Y_test = to_one_hot(test['label'].values, num_classes = 10)\n",
    "X_train = train.drop(labels = ['label'],axis = 1).values/255.0\n",
    "X_test= test.drop(labels = ['label'],axis = 1).values/255.0\n",
    "X_train.shape,Y_train.shape,X_test.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "learning_rate = 0.01\n",
    "training_iteration = 30\n",
    "batch_size = 128\n",
    "display_step = 2\n",
    "\n",
    "# TF graph input\n",
    "x = tf.placeholder(tf.float64, [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float64, [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "x_list=tf.split(x, len(workers))\n",
    "y_list=tf.split(y, len(workers))\n",
    "\n",
    "# Create a model\n",
    "with tf.device(param_servers[0]):\n",
    "    W_1 = tf.Variable(tf.truncated_normal([784, 64],dtype=tf.float64))\n",
    "    b_1 = tf.Variable(tf.zeros([64],dtype=tf.float64))\n",
    "    \n",
    "with tf.device(param_servers[1]):\n",
    "    W_2 = tf.Variable(tf.truncated_normal([64, 10],dtype=tf.float64))\n",
    "    b_2 = tf.Variable(tf.zeros([10],dtype=tf.float64))\n",
    "\n",
    "losses=[]\n",
    "accuracies=[]\n",
    "for i,worker in enumerate(workers):\n",
    "    with tf.device(worker):    \n",
    "        # Construct a linear model\n",
    "        h_1=tf.nn.relu(tf.matmul(x_list[i], W_1) + b_1)\n",
    "        h_2=tf.matmul(h_1, W_2) + b_2\n",
    "        model = tf.nn.softmax(h_2)  # Softmax\n",
    "\n",
    "        loss = -tf.reduce_sum(y_list[i]*tf.log(model))\n",
    "        losses.append(loss)\n",
    "\n",
    "        predictions = tf.equal(tf.argmax(model, 1), tf.argmax(y_list[i], 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(predictions, \"float\"))\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "average_loss_op = tf.add_n(losses) / tf.convert_to_tensor(len(losses), dtype=tf.float64)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "\n",
    "average_accuracy_op = tf.add_n(accuracies) / tf.convert_to_tensor(len(accuracies), dtype=tf.float32)\n",
    "\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:0 training loss= 71.135869 training accuracy= 0.640943 test accuracy= 0.714381\n",
      "Iteration:1 training loss= 25.506088 training accuracy= 0.778150 test accuracy= 0.777048\n",
      "Iteration:2 training loss= 20.917178 training accuracy= 0.821458 test accuracy= 0.798190\n",
      "Iteration:3 training loss= 18.625326 training accuracy= 0.843299 test accuracy= 0.815048\n",
      "Iteration:4 training loss= 17.285545 training accuracy= 0.860477 test accuracy= 0.825714\n",
      "Iteration:5 training loss= 16.368682 training accuracy= 0.871304 test accuracy= 0.837429\n",
      "Iteration:6 training loss= 15.712874 training accuracy= 0.878796 test accuracy= 0.846952\n",
      "Iteration:7 training loss= 15.338522 training accuracy= 0.884383 test accuracy= 0.854095\n",
      "Iteration:8 training loss= 15.020151 training accuracy= 0.888824 test accuracy= 0.860381\n",
      "Iteration:9 training loss= 14.827269 training accuracy= 0.892820 test accuracy= 0.862667\n",
      "Iteration:10 training loss= 14.692618 training accuracy= 0.896345 test accuracy= 0.864667\n",
      "Iteration:11 training loss= 14.641586 training accuracy= 0.898793 test accuracy= 0.867905\n",
      "Iteration:12 training loss= 14.711807 training accuracy= 0.899936 test accuracy= 0.871905\n",
      "Iteration:13 training loss= 14.782530 training accuracy= 0.900759 test accuracy= 0.874191\n",
      "Iteration:14 training loss= 14.851575 training accuracy= 0.902062 test accuracy= 0.878000\n",
      "Iteration:15 training loss= 14.975757 training accuracy= 0.903649 test accuracy= 0.878667\n",
      "Iteration:16 training loss= 15.144413 training accuracy= 0.905777 test accuracy= 0.880476\n",
      "Iteration:17 training loss= 15.300204 training accuracy= 0.906190 test accuracy= 0.881905\n",
      "Iteration:18 training loss= 15.455014 training accuracy= 0.907619 test accuracy= 0.882286\n",
      "Iteration:19 training loss= 15.625198 training accuracy= 0.908508 test accuracy= 0.883048\n",
      "Iteration:20 training loss= 15.750505 training accuracy= 0.909525 test accuracy= 0.883810\n",
      "Iteration:21 training loss= 15.922236 training accuracy= 0.910856 test accuracy= 0.884381\n",
      "Iteration:22 training loss= 16.035400 training accuracy= 0.911104 test accuracy= 0.886095\n",
      "Iteration:23 training loss= 16.205228 training accuracy= 0.912025 test accuracy= 0.886190\n",
      "Iteration:24 training loss= 16.413284 training accuracy= 0.913169 test accuracy= 0.887048\n",
      "Iteration:25 training loss= 16.607037 training accuracy= 0.912915 test accuracy= 0.886667\n",
      "Iteration:26 training loss= 16.789433 training accuracy= 0.913264 test accuracy= 0.888952\n",
      "Iteration:27 training loss= 17.011559 training accuracy= 0.914090 test accuracy= 0.890000\n",
      "Iteration:28 training loss= 17.223512 training accuracy= 0.914630 test accuracy= 0.890952\n",
      "Iteration:29 training loss= 17.372911 training accuracy= 0.914757 test accuracy= 0.890857\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session(server.target) as sess:\n",
    "    sess.run(init)\n",
    "    # Training cycle\n",
    "    epsilon=0.9\n",
    "    test_acc = 0.0\n",
    "    iteration=0\n",
    "    while test_acc < epsilon and iteration < 30:\n",
    "        avg_acc = 0.0\n",
    "        avg_cost = 0.0\n",
    "        for i in range(num_batches):\n",
    "            lower_index=batch_size*i\n",
    "            if i==num_batches-1:\n",
    "                batch = X_train[lower_index:]\n",
    "                batch_labels=Y_train[lower_index:]\n",
    "            else:\n",
    "                upper_index=batch_size*(i+1)\n",
    "                batch = X_train[lower_index:upper_index]\n",
    "                batch_labels=Y_train[lower_index:upper_index]\n",
    "            \n",
    "            sess.run(train_step, feed_dict={x: batch, y: batch_labels})\n",
    "            \n",
    "            avg_cost += sess.run(average_loss_op, feed_dict={x: batch, y: batch_labels})/float(num_batches)\n",
    "            avg_acc += sess.run(average_accuracy_op, feed_dict={x: batch, y: batch_labels})/float(num_batches)\n",
    "            \n",
    "        test_loss = sess.run(average_loss_op, feed_dict={x: X_test, y: Y_test})\n",
    "        test_acc = sess.run(average_accuracy_op, feed_dict={x: X_test, y: Y_test})\n",
    "        \n",
    "        print (\"Iteration:%d\"%iteration, \"training loss= %f\"%avg_cost, \"training accuracy= %f\"%avg_acc,\\\n",
    "               \"test accuracy= %f\"%test_acc)\n",
    "        iteration+=1\n",
    "        \n",
    "    print(\"Training completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
